\documentclass{beamer}
\usepackage{proof}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{graphicx}
\title[A Graph-Free Approach to Data-flow Analysis]
{A Graph-Free Approach to Data-flow Analysis \nocite{Mohnen}}
\date{October 3, 2012}

\author[Markus Mohnen]{Markus Mohnen}
\begin{document}

\AtBeginSection[]
{
    \begin{frame}{Table of Contents}
        \tableofcontents[currentsection]
    \end{frame}
}

\begin{frame}
\titlepage
\begin{center}
Presented by Ismail Badawi
\end{center}
\end{frame}

\begin{frame}{Table of Contents}
\tableofcontents
\end{frame}

\section{Overview}
\begin{frame}{What are we talking about?}
\begin{itemize}
\item A Graph-free Approach to Data-flow Analysis, published in CC 2002
\item Traditionally (and as we saw in COMP621), data-flow analysis is done
using an algorithm based on propagating data-flow facts along edges in
a control-flow graph and computing a fixed point
\item This paper presents an alternative algorithm that is based on
abstract interpretation -- it operates on an array of instructions and
doesn't need an explicit graph
\item This is good because memory usage goes down, and it's also better
for cache locality
\end{itemize}
\end{frame}

\begin{frame}{What are we trying to do?}
\begin{itemize}
\item At a very high level, we want to examine the semantics (roughly, 
the behavior) of a program
\item The concrete semantics: execution traces, i.e. the complete state
of execution (e.g. values for all variables) at every program point
\item Not always practical to deal with traces
\item Abstract interpretation: instead of considering the concrete
semantics, introduce \emph{abstract semantics} that model the behavior
of a program without executing it
\item Model a program as a set of abstract values $A$, and assign to each
kind of instruction a function on $A$ (called an abstraction)
\end{itemize}
\end{frame}

\section{Data-flow problems, formally}
\begin{frame}{Instructions}
The programs considered are three-address code programs; instructions are
\begin{itemize}
\item Conditional jumps ({\tt if} $\psi$ {\tt goto n})
\item Unconditional jumps ({\tt goto n})
\item Assignments ({\tt x := y op z})
\end{itemize}
This just makes the presentation simpler. A real implementation
targeting JVM instructions is also provided.
\end{frame}     

\begin{frame}{Refresher: lattices}
\begin{itemize}
\item A \emph{lattice} is a partially ordered set in which any two 
elements have a least upper bound (\emph{join}, $\sqcup$) and a greatest
lower bound (\emph{meet}, $\sqcap$).
\item Fixed point theorem: Any monotonic function $f$ on a finite lattice 
$D$ has a unique maximum fixed point $\sqcap_i f^i(\bot)$, obtained after
finitely many iterations.
\end{itemize}
\end{frame}
 
\begin{frame}{Data-flow problems}
A data-flow problem is a quadruple $(P, L, ![.!], a_0)$, where
\begin{itemize}
\item $P$ is a program, a sequence of instructions $I_0,\dots,I_n \in 
Instr$
\item $L = \langle A, \sqcap, \sqcup \rangle$ is a lattice, where $A$ is
a set of abstract values (different for each problem), and $\sqcap$ and 
$\sqcup$ are binary meet and join operations on $A$
\item $![.!] : Instr \rightarrow (A \rightarrow A)$ is an abstract
semantics, describing how the instructions operate on the abstract values
in $A$
\item $a_0 \in A$ is an initial value for the entry of $P$
\end{itemize}
\end{frame}

\begin{frame}{Solutions to data-flow problems}
Given a program $P = I_0,\dots,I_n$, define the predecessor
function $pred_P: \{0,\dots,n\} \rightarrow \mathcal{P}(\{0,\dots,n\})$. $j \in pred_P(i)$ if:
\begin{itemize}
\item $I_j \in \{${\tt goto i}$, \, ${\tt if} $\psi$ {\tt goto i}$\} \text{, or }$
\item $i = j + 1 \text{ and } I_j \neq \, ${\tt goto t}
\end{itemize}
We're looking for a vector of abstract values $s_0,\dots,s_n \in A$ such
that $s_i = \sqcap_{j \in pred_P(i)} ![I_j!](s_j)$. If we assume our
semantics is monotone, then by the fixed point
theorem, this exists. $s_i$ is the value valid just before $I_i$.
\end{frame}

\begin{frame}{Running example: constant propagation}
Constant propagation aims at finding as many constants as possible at
compile time, so as to replace computations with constant values.
\begin{itemize}
\item Let $C = \mathbb{Z} \cup \{\top, \bot\}$, with
a relation $c_1 \leq c_2 \iff c_1 = c_2 \vee c_1 = \bot \vee c_2 = \top$,
and let $X$ be the set of variables in $P$.
\item The abstract values are functions $X \rightarrow C$
\item The abstract semantics $![.!] : Instr \rightarrow (A \rightarrow A)$
are defined by 
\begin{itemize}
\item $![${\tt goto l}$!] = \, ![${\tt if} $\psi$ {\tt goto l}$!] = id$
\item $![${\tt x := y op z}$!] = c \mapsto c'$, where $c' = c[x/a]$, i.e.
same function except at $x$
\item $c'(x) = a := \begin{cases}
a_y \, op \, a_z & \text{if } y = a_y \in \mathbb{Z} \text{ or } c(y) = a_y \in \mathbb{Z} \\
& \text{and } z = a_z \in \mathbb{Z} \text{ or } c(z) = a_z \in \mathbb{Z} \\
\bot & \text{otherwise}
\end{cases}$
\end{itemize}
\item The initial value for the entry of $P$ is 
$x \mapsto \bot \, \forall \, x$
\end{itemize}
\end{frame}

\begin{frame}{Question for you}
To make sure we understand this way of specifying a data-flow problem by
describing abstract values and the way the instructions act on them, let's
state another well-known data-flow problem this way: reaching definitions.
Describe (a) the abstract values (b) the abstract semantics

\begin{enumerate}[(a)]
\pause \item Sets of definitions (instruction labels)
\pause \item The abstract semantics are:
\begin{itemize}
\item $![${\tt goto l}$!] = \, ![${\tt if} $\psi$ {\tt goto l}$!] = id$
\item $![${\tt d: x := y op z}$!] = c \mapsto \{d\} \cup (c \setminus defs(x))$
\end{itemize}
\end{enumerate}
Similar to the $gen$ and $kill$ sets presentation we saw; can think
of the semantics mapping in sets to out sets
\end{frame}

\begin{frame}{Another question for you}
The presentation here is describing forward / must problems. What has to
change to deal with (a) backward problems (b) may problems?
\begin{enumerate}[(a)]
\pause
\item Consider $succ_P$ instead of $pred_P$; $i \in succ_P(j) \iff j \in pred_P(i)$
\pause
\item Change $\sqcap$ to $\sqcup$; this computes the least fixed point
instead of the greatest fixed point
\end{enumerate}
\end{frame}

\begin{frame}{Running example program}
\begin{tiny}
\begin{tabular}{l l l}
Instructions & Abstraction & Solution \\
$I_0$: {\tt x := 1} & $x/1$ & $x \mapsto \bot, y \mapsto \bot, z \mapsto \bot, r \mapsto \bot$ \\
$I_1$: {\tt y := 2} & $y/2$ & $x \mapsto 1, y \mapsto \bot, z \mapsto \bot, r \mapsto \bot$\\
$I_2$: {\tt z := 3} & $z/3$ & $x \mapsto 1, y \mapsto 2, z \mapsto \bot, r \mapsto \bot$\\
$I_3$: {\tt goto 8} & (identity) & $x \mapsto 1, y \mapsto 2, z \mapsto 3, r \mapsto \bot$\\
$I_4$: {\tt r := y + z} & $r/ \begin{cases} c(y) + c(z) & c(y), c(z) \in \mathbb{Z} \\
\bot & \text{otherwise}
\end{cases}$ & $x \mapsto \bot, y \mapsto 2, z \mapsto 3, r \mapsto \bot$\\ 
$I_5$: {\tt if x <= z goto 7} & (identity) & $x \mapsto \bot, y \mapsto 2, z \mapsto 3, r \mapsto 5$ \\
$I_6$: {\tt r := z + y} & $r/ \begin{cases} c(z) + c(y) & c(y), c(z) \in \mathbb{Z} \\
\bot & \text{otherwise}
\end{cases}$ & $x \mapsto \bot, y \mapsto 2, z \mapsto 3, r \mapsto 5$\\
$I_7$: {\tt x := x + 1} & $x/ \begin{cases} c(x) + 1 & c(x) \in \mathbb{Z} \\
\bot & \text{otherwise}
\end{cases}$ & $x \mapsto \bot, y \mapsto 2, z \mapsto 3, r \mapsto 5$\\
$I_8$: {\tt if x < 10 goto 4} & (identity) & $x \mapsto \bot, y \mapsto 2, z \mapsto 3, r \mapsto 5$\\
\end{tabular}
\end{tiny}
\end{frame}

\section{Classical graph-based algorithm}
\begin{frame}[fragile]{Modeling control flow using graphs}
This should be familiar, so let's skip most of the formalism. 
Given a program $P$, the $pred_P$ function we
defined earlier defines a graph, the \emph{single-instruction graph} 
$SIG(P)$:

\tikzstyle{block} = [rectangle, draw]
\begin{tikzpicture} [node distance = 1cm, auto]
\node[block] (i0) {$I_0$};
\node[block, right of=i0] (i1) {$I_1$} edge [<-] (i0);
\node[block, right of=i1] (i2) {$I_2$} edge [<-] (i1);
\node[block, right of=i2] (i3) {$I_3$} edge [<-] (i2);
\node[block, right of=i3] (i4) {$I_4$} edge [<-] (i3);
\node[block, right of=i4] (i5) {$I_5$} edge [<-] (i4);
\node[block, right of=i5] (i6) {$I_6$} edge [<-] (i5);
\node[block, right of=i6] (i7) {$I_7$} edge [<-] (i6);
\node[block, right of=i7] (i8) {$I_8$} edge [<-] (i7);
\draw [->] (i3) to [out=90, in=90] (i8);
\draw [->] (i5) to [out=90, in=90] (i7);
\draw [->] (i8) to [out=270, in=270] (i4);
\end{tikzpicture}
\\ Often, we want information for each program point, so this is useful,
but not as efficient as it could be.
\end{frame}

\begin{frame}[fragile]{Basic-block graphs}
By merging together maximal sequences of straight-line code, we get
basic blocks. The \emph{basic block graph} $BBG(P)$:
\tikzstyle{block} = [rectangle, draw]
\begin{tikzpicture} [node distance = 2.3cm, auto]
\node[block] (b0) {$B_0 : [I_0 I_1 I_2 I_3] $};
\node[block, right of=b0] (b1) {$B_1: [I_4 I_5]$} edge [<-] (b0);
\node[block, right of=b1] (b2) {$B_2: [I_6]$} edge [<-] (b1);
\node[block, right of=b2] (b3) {$B_3: [I_7]$} edge [<-] (b2);
\node[block, right of=b3] (b4) {$B_4: [I_8]$} edge [<-] (b3);
\draw [->] (b0) to [out=90, in=90] (b4);
\draw [->] (b1) to [out=90, in=90] (b3);
\draw [->] (b4) to [out=270, in=270] (b1);
\end{tikzpicture}  
\end{frame}

\begin{frame}{Basic block semantics}
We also extend the abstract semantics, which are defined on instructions,
to basic blocks in a straightforward way using function composition:
$$ ![I_0 \dots I_n!] := \, ![I_n!] \circ \dots \circ ![I_0!] $$
\end{frame}

\begin{frame}{The algorithm: basic idea}
\begin{itemize}
\item Uses $BBG(P)$, a working set $W$ of nodes to visit, and an
associative array $a$ mapping nodes to abstract values.
\item Initialize $W$ to contain all nodes. While $W$ is not empty:
\item Pick a node $B$ from $W$ and remove it. (Many ways to do this, assume
some non-deterministic choice)
\item Compute a new abstract value for $B$ based on its current value and
the values of its predecessors
\item If this new value is different, update the old one, and add all of 
$B$'s successors to $W$.
\end{itemize}
Once we're done, we can propagate the information back to the level of
single instructions.
\end{frame}

\begin{frame}{The algorithm: initialization}
Input: $(P = (I_0,\dots,I_n), L = \langle A, \sqcap, \sqcup, \rangle, ![.!], a_0)$ \\
Output: $s_0, \dots, s_n \in A$ 

\bigskip
{\tt G = (BB(P), E, B0) := BBG(P)} \\
{\tt a[B0] := a0} \\
{\tt for each B} $\in$ {\tt BB(P)} $\setminus$ {\tt B0 do a[B] :=} $\top$ \\
{\tt W := BB(P)}
\end{frame}

\begin{frame}{The algorithm: main loop}
\smallskip
{\tt while W} $\neq \varnothing$ {\tt do} \\
$\, \,$ {\tt choose B} $\in$ {\tt W} \\
$\, \,$ {\tt W := W} $\setminus$ {\tt B} \\
$\, \,$ {\tt new := a[B]} \\
$\, \,$ {\tt for each B'} $\in$ {\tt pred(B) do new := new} $\sqcap$ {\tt ![B'!](a[B'])} \\
$\, \,$ {\tt if new} $\neq$ {\tt a[B] then} \\
$\, \, \, \, \, \,$ {\tt a[B] := new} \\
$\, \, \, \, \, \,$ {\tt W := W} $\cup$ {\tt succ(B)} \\ 
$\, \,$ {\tt end} \\
{\tt end}
\end{frame}

\begin{frame}{The algorithm: post-processing}
\smallskip
{\tt for each B} $\in$ {\tt BB(P) do} \\
$\, \,$ {\tt with B = I[k...l] do} \\
$\, \, \, \, \, \,$ {\tt s[k] := a[B]} \\
$\, \, \, \, \, \,$ {\tt for i := k + 1 to l do s[i] := ![I[i-1]!](s[i-1])} \\
$\, \,$ {\tt end} \\
{\tt end}
\end{frame}

\begin{frame}{Execution on our example}
\includegraphics[scale=0.35]{classic.png}
\end{frame}

\section{New evaluation-based algorithm}
\begin{frame}{The algorithm}
\begin{itemize}
\item Uses an array of instructions $I$, a working set of program counters
$W$, and a parallel array of abstract values $a$
\item Initialize $W$ to $\{0,\dots,n\}$. While $W$ is not empty:
\item Pick a program counter $pc \in W$
\item Propagate abstract values along an entire path of computation starting
at $pc$, until values don't change or $pc = n + 1$; remove each visited
$pc$ from $W$ as you go
\end{itemize}
No post-processing necessary since we're already dealing with instructions.
\end{frame}

\begin{frame}{The algorithm: initialization}
\smallskip
{\tt s[0] = a0} \\
{\tt for i := 1 to n do s[i] =} $\top$ \\
{\tt W := set(0, ..., n)}
\end{frame}

\begin{frame}{The algorithm: main loop}
\smallskip
\begin{footnotesize}
{\tt while W} $\neq \varnothing$ {\tt do} \\
$\, \,$ {\tt choose pc} $\in$ {\tt W} \\
$\, \,$ {\tt repeat} \\
$\, \, \, \, \, \,$ {\tt W := W} $\setminus$ {\tt pc} \\
$\, \, \, \, \, \,$ {\tt new := ![I[pc]!](s[pc])} \\
$\, \, \, \, \, \,$ {\tt if I[pc] = (goto l) then pc' := l} \\
$\, \, \, \, \, \,$ {\tt else} \\
$\, \, \, \, \, \, \, \, \, \, \, \,$ {\tt pc' := pc + 1} \\
$\, \, \, \, \, \, \, \, \, \, \, \,$ {\tt if I[pc] = (if} $\psi$ {\tt goto l) and new < s[l] then} \\
$\, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \,$ {\tt W := W + l} \\
$\, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \, \,$ {\tt s[l] := new} \\
$\, \, \, \, \, \, \, \, \, \, \, \,$ {\tt end} \\
$\, \, \, \, \, \,$ {\tt end} \\
$\, \, \, \, \, \,$ {\tt if new < s[pc'] then} \\
$\, \, \, \, \, \, \, \, \, \, \, \,$ {\tt s[pc'] := new} \\
$\, \, \, \, \, \, \, \, \, \, \, \,$ {\tt pc := pc'} \\
$\, \, \, \, \, \,$ {\tt else pc := n + 1} \\
$\, \, \, \, \, \,$ {\tt end} \\
$\, \,$ {\tt until pc = n + 1} \\
$\, \,$ {\tt end} \\
{\tt end} \\
\end{footnotesize}
\end{frame}

\begin{frame}{Execution on our example}
\includegraphics[scale=0.35]{bb.png}
\end{frame}

\begin{frame}{Last question}
Really the same question as before, but in terms of the code here. What
has to change here to handle (a) may problems and (b) backward problems?
\begin{enumerate}[(a)]
\pause
\item Replace both $<$ by $>$
\pause
\item This one is more complicated. You could add some synthetic
instructions connecting jump targets to the originating jump
\end{enumerate}
\end{frame}

\section{Implementation on the JVM}

\begin{frame}{Infrastructure}
\begin{itemize}
\item Implemented as part of the jDFA framework, using the bytecode
engineering library (BCEL)
\item Defines an {\tt Abstraction} interface to represent the behavior
of instructions on abstract values. Main methods:
\begin{itemize}
\item {\tt Function getAbstract(InstructionHandle ih)}
\item {\tt Object getInitialValue(InstructionHandle ih)}
\item {\tt Lattice getLattice()}
\end{itemize}
where {\tt Function} is a class that holds domain and range sets, and
provides an {\tt apply} method.
\item Defines an abstract {\tt Solver} class that takes an {\tt Abstraction}
and an {\tt InstructionList} and provides a {\tt getSolution()} method;
implementations -- {\tt FlowGraphSolver} and {\tt ExecutionSolver}
\end{itemize}
\end{frame}

\begin{frame}{Sample {\tt Abstraction}}
\begin{itemize}
\item {\tt CFPAbstraction} (CFP: Constant folding propagation)
\item Details kind of complicated; uses a {\tt Lattice} to describe the
state of local variable slots, and another to describe the state of the
stack
\item BCEL provides a class hierarchy for JVM instructions -- common
superclass {\tt ArithmeticInstruction} represents instructions like
{\tt dadd, isub, fmul, ldiv}, $\dots$, {\tt ConversionInstruction}
represents instructions like {\tt l2i, i2d}, $\dots$
\item Basically implements {\tt getAbstract()} as a big if-else chain
of {\tt instanceof} checks; in each case return a {\tt Function} modeling
that kind of instruction
\item Intraprocedural: for method calls, assign $\top$ to return value.
\end{itemize}
\end{frame}
\section{Evaluation}

\begin{frame}{Experimental framework}
\begin{itemize}
\item Large set of samples (15339 classes, total of 98947 methods), 
including the JRE (1.2), ANTLR, BCEL, compiled using various compilers
\item Also established a web site where people could donate class files
\end{itemize}
The new algorithm is expected to behave better memory-wise. On the other
hand, because it doesn't deal with basic blocks, it's expected to have
worse performance, especially in cases with huge basic block reductions.
\end{frame}

\begin{frame}{Memory}
\begin{itemize}
\item For each method, measured $\frac{m_X}{m_c} \cdot 100$, where $m_X$ is
the number of bytes allocated by the execution algorithm, and $m_C$ is the
number of bytes allocated by the classic algorithm; lower is better
\item Reductions ranging from $74.61\%$ to $7.28\%$, with an average of
$30.83\%$ and a median of $31.28\%$. Algorithm \emph{always} reduced
memory usage!
\end{itemize}
\end{frame}

\begin{frame}{Memory (visually)}
\includegraphics[scale=0.35]{histogram.png} \\
\includegraphics[scale=0.35]{memory.png}
\end{frame}

\begin{frame}{Performance}
\begin{itemize}
\item Measured the speedup $\frac{t_C}{t_X}$, where $t_C$ is the runtime
of the classic algorithm and $t_X$ of the execution algorithm; higher is
better
\item Huge distribution; speedups ranging from $291.2$ down to $0.015$.
Average was $1.62$, median $1.33$, variance $7.49$ (which is low)
\item Conclusion: for a majority of methods, the new algorithm performs as
well as the classical algorithm
\item (And speedups \emph{are} higher with better basic block reductions.)
\end{itemize}
\end{frame}

\begin{frame}{Performance (visually)}
\includegraphics[scale=0.35]{histogram2.png} \\
\includegraphics[scale=0.35]{performance.png}
\end{frame}

\begin{frame}{References}
\bibliographystyle{plain}
{\footnotesize
\bibliography{bib}}
\end{frame}

\end{document}
